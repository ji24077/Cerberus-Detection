{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":982,"status":"ok","timestamp":1720059830634,"user":{"displayName":"Hana Truchla","userId":"05723343436954264366"},"user_tz":240},"id":"fEKbv1zNMH5q","outputId":"6d7adc2a-cef7-44f6-c033-b63717bb1651"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03ZC-i0GVxLv"},"outputs":[],"source":["#REMOVING DUPLICATES IN THE CSV FILES\n","import pandas as pd\n","def remove_duplicates(source, dest):\n","\n","  data = pd.read_csv(source)\n","  data = data.drop_duplicates(subset=['text'])\n","  data.to_csv(dest, index=False)\n","\n","  return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kk4ldrLvJh8r"},"outputs":[],"source":["#SPLITTING UP THE DATA INTO 60% TRAINING, 20% VALIDATION AND 20% TESTING\n","import os\n","import numpy as np\n","import pandas as pd\n","import csv\n","import random\n","\n","#in original csv file\n","#ROW 1 THROUGH 25996 is 0 -> human generated\n","#ROW 25997 THROUGH 65508 is 1 -> AI generated\n","\n","def split_data(source, train_dest, val_dest, test_dest, train_size = 0.6, val_size = 0.2):\n","\n","    #Data is already sorted beginning with 0 -> human generated\n","    #find the index at which point 1 begins -> AI generated\n","\n","    #split the data\n","    Human_data = []\n","    AI_data = []\n","\n","    with open(source, newline='') as csvfile:\n","      reader = csv.reader(csvfile, delimiter=',')\n","      next(reader, None)\n","\n","      for row in reader:\n","        text, category = row[0], row[1]\n","        if category == '0':\n","          Human_data.append([text, category])\n","        elif category == '1':\n","          AI_data.append([text, category])\n","\n","    #find indices to split the data for 60% train, 20% val, 20% test to ensure there is equal amount of AI and human data\n","    if len(Human_data) > len(AI_data):\n","      index = len(AI_data)\n","    else:\n","      index = len(Human_data)\n","\n","    #make seperate lists for train, val and test data sets\n","    train_data = Human_data[:round(index * train_size)] + AI_data[:round(index * train_size)]\n","    val_data = Human_data[round(index*train_size):round(index * (train_size + val_size))] + AI_data[round(index*train_size):round(index * (train_size + val_size))]\n","    test_data = Human_data[round(index * (train_size + val_size)):] + AI_data[round(index * (train_size + val_size)):len(Human_data)]\n","\n","    print(val_data)\n","\n","    #shuffle the data sets so all AI don't appear right after each other vice versa for humans\n","    random.shuffle(train_data)\n","    random.shuffle(val_data)\n","    random.shuffle(test_data)\n","\n","    #create 3 seperate csv files -> one for each data set (train, val, test)\n","    df_train = pd.DataFrame(train_data, columns=['text', 'category'])\n","    df_train.to_csv(train_dest, index=False)\n","\n","    df_val = pd.DataFrame(val_data, columns=['text', 'category'])\n","    df_val.to_csv(val_dest, index=False)\n","\n","    df_test = pd.DataFrame(test_data, columns=['text', 'category'])\n","    df_test.to_csv(test_dest, index=False)\n","\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lv4QYICbZQF7"},"outputs":[],"source":["#take the csvfile containing the data and output a list of the form\n","# [ [ [word1], [word2], ...], category], [ [word1], [word2], ...], category], ...]\n","\n","def csv_to_list(source):\n","\n","  df = pd.read_csv(source)\n","\n","  output_list = []\n","\n","  for i, row in df.iterrows():\n","    text = row['text']\n","    category = row['category']\n","    if type(text) == str:\n","      words = text.split()\n","    output_list.append([words, category])\n","\n","  return output_list\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPfiA8I9fwQB"},"outputs":[],"source":["#find the average length of the each text\n","\n","def average_length(source):\n","\n","  lengths = []\n","\n","  for i in range(len(source)):\n","    lengths.append(len(source[i][0]))\n","\n","  return sum(lengths)/len(lengths)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZOA7julZ7t_"},"outputs":[],"source":["\n","#RUNNING THE TEXT PREPROCESSING AND DATA SPLITTING USING THE ABOVE FUNCTIONS\n","\n","#removing the duplicates in the data\n","original_source = '/content/gdrive/MyDrive/ASP360Project/Data/train_v3_drcat_01.csv'\n","no_duplicates_source = '/content/gdrive/MyDrive/ASP360Project/Data/data_noduplicates.csv'\n","\n","remove_duplicates(original_source,no_duplicates_source)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1efWUp8_7NhoqbWF-uMTGM_RHjiOhehBg"},"executionInfo":{"elapsed":38061,"status":"ok","timestamp":1720060266679,"user":{"displayName":"Hana Truchla","userId":"05723343436954264366"},"user_tz":240},"id":"gG7a3_3oczkD","outputId":"a2445d14-2aff-4194-9267-79ad8d497c4d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#splitting up the data into train, val, test\n","\n","train_dest = '/content/gdrive/MyDrive/ASP360Project/Data/train.csv'\n","val_dest = '/content/gdrive/MyDrive/ASP360Project/Data/val.csv'\n","test_dest = '/content/gdrive/MyDrive/ASP360Project/Data/test.csv'\n","\n","split_data(no_duplicates_source, train_dest, val_dest, test_dest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvEelC7mfXeF"},"outputs":[],"source":["#changing the text format from a paragraph to a list of words\n","\n","train_data = csv_to_list(train_dest)\n","\n","test_data = csv_to_list(test_dest)\n","\n","val_data = csv_to_list('/content/gdrive/MyDrive/ASP360Project/Data/val.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1720053360350,"user":{"displayName":"Viv Huynh","userId":"17059720709335226542"},"user_tz":240},"id":"AlcX-sXHgE8L","outputId":"e8739ed3-731a-44f6-c7fd-a14812119ea1"},"outputs":[{"name":"stdout","output_type":"stream","text":["360.61209888570903\n"]}],"source":["#find the average word count of the essays\n","\n","average_word_count =( average_length(train_data) + average_length(val_data) + average_length(test_data) )/3\n","\n","print(average_word_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176,"status":"ok","timestamp":1720046764744,"user":{"displayName":"Viv Huynh","userId":"17059720709335226542"},"user_tz":240},"id":"Mwb5eAasgrm0","outputId":"c1234e93-c13d-4168-ae69-84d99591af6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["32846\n","32846\n","32846\n"]}],"source":["print(len(train_data))\n","print(len(val_data))\n","print(len(test_data))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}